{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from collections import namedtuple\n",
        "from IPython.display import clear_output, display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define replay memory\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        if len(input_shape) == 1:  # Assuming input shape for fully connected layers\n",
        "            self.fc1 = nn.Linear(input_shape[0], 128)\n",
        "            self.fc2 = nn.Linear(128, 128)\n",
        "            self.fc3 = nn.Linear(128, num_actions)\n",
        "        elif len(input_shape) == 3:  # Assuming input shape for convolutional layers\n",
        "            self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
        "            self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "            self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "            conv_out_size = self._get_conv_out(input_shape)\n",
        "            self.fc1 = nn.Linear(conv_out_size, 512)\n",
        "            self.fc2 = nn.Linear(512, num_actions)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported input shape\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        if hasattr(self, 'conv1'):  # Using convolutional layers\n",
        "            x = F.relu(self.conv1(x))\n",
        "            x = F.relu(self.conv2(x))\n",
        "            x = F.relu(self.conv3(x))\n",
        "            x = F.relu(self.fc1(x.view(x.size(0), -1)))\n",
        "        else:  # Using fully connected layers\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = F.relu(self.fc2(x))\n",
        "            x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv1(torch.zeros(1, *shape))\n",
        "        o = self.conv2(o)\n",
        "        o = self.conv3(o)\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "# Define training function\n",
        "def train_dqn(env_name, num_episodes, batch_size, gamma, eps_start, eps_end, eps_decay, target_update):\n",
        "    env = gym.make(env_name)\n",
        "    memory = ReplayMemory(10000)\n",
        "    policy_net = DQN(env.observation_space.shape, env.action_space.n)\n",
        "    target_net = DQN(env.observation_space.shape, env.action_space.n)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    target_net.eval()\n",
        "    optimizer = optim.Adam(policy_net.parameters())\n",
        "    steps_done = 0\n",
        "    rewards = []\n",
        "\n",
        "    def select_action(state, epsilon):\n",
        "        if random.random() > epsilon:\n",
        "            with torch.no_grad():\n",
        "                return policy_net(state).max(1)[1].view(1, 1)\n",
        "        else:\n",
        "            return torch.tensor([[random.randrange(env.action_space.n)]], dtype=torch.long)\n",
        "\n",
        "    def optimize_model():\n",
        "        if len(memory) < batch_size:\n",
        "            return\n",
        "        transitions = memory.sample(batch_size)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
        "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "        state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "        next_state_values = torch.zeros(batch_size)\n",
        "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "        expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
        "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        for param in policy_net.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "        optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    epsilon = eps_start\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        state = torch.tensor([state], dtype=torch.float32)\n",
        "        total_reward = 0\n",
        "        for t in range(10000):\n",
        "            action = select_action(state, epsilon)\n",
        "            next_state, reward, done, _ = env.step(action.item())\n",
        "            next_state = torch.tensor([next_state], dtype=torch.float32) if not done else None\n",
        "            reward = torch.tensor([reward], dtype=torch.float32)\n",
        "            memory.push(state, action, next_state, reward)\n",
        "            state = next_state\n",
        "            total_reward += reward.item()\n",
        "            loss = optimize_model()\n",
        "            steps_done += 1\n",
        "            if done:\n",
        "                rewards.append(total_reward)\n",
        "                break\n",
        "        if episode % target_update == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "        if epsilon > eps_end:\n",
        "            epsilon *= eps_decay\n",
        "\n",
        "    env.close()\n",
        "    return rewards\n",
        "\n",
        "# Parameters\n",
        "env_name = 'CartPole-v1'\n",
        "num_episodes = 1000\n",
        "batch_size = 128\n",
        "gamma = 0.99\n",
        "eps_start = 0.9\n",
        "eps_end = 0.05\n",
        "eps_decay = 0.995\n",
        "target_update = 10\n",
        "\n",
        "# Train DQN\n",
        "rewards = train_dqn(env_name, num_episodes, batch_size, gamma, eps_start, eps_end, eps_decay, target_update)\n",
        "\n",
        "#--2nd one\n",
        "\n",
        "def play_atari(env_name, num_episodes):\n",
        "    env = gym.make(env_name)\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            plt.imshow(env.render(mode='rgb_array'))\n",
        "            plt.axis('off')\n",
        "            clear_output(wait=True)\n",
        "            display(plt.gcf())\n",
        "            action = env.action_space.sample()\n",
        "            state, reward, done, _ = env.step(action)\n",
        "    env.close()\n",
        "\n",
        "# Call the gameplay display function\n",
        "play_atari(env_name, num_episodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "J3r4pJcM_kBO",
        "outputId": "9419b567-59f7-43fe-bf05-68a5c73d92bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}